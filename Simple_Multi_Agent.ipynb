{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-BILAL-5/simple-multi-agents-workflow/blob/main/Simple_Multi_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade langgraph langchain langchain_community langchain-chroma langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqZWuO5rC7gZ",
        "outputId": "1129f4e8-b7d0-45ef-e391-1cf76ce16faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Gemini_Api_Key')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"langchai_api_key\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-demo\""
      ],
      "metadata": {
        "id": "Ql1dCg2SB09_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4iSRUzCbZnT",
        "outputId": "ace09afd-d0a9-425a-adfe-d293649ba9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://python.langchain.com/docs/tutorials/query_analysis/\")\n",
        "docs = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv5w-tZ0gI4j"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwVkWl_5b4Da"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbAjNZdofZ_6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(texts, embeddings )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONZZmHWfhems"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import MultiQueryRetriever\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash-8b\")\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=db.as_retriever(),\n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL97Yercl1FC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "0c77c146-7c51-47bf-a962-47c1399cd28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page Not Found | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchPage Not FoundWe could not find what you were looking for.Please contact the owner of the site that linked you to the original URL and let them know their link is broken.CommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    db.as_retriever(),\n",
        "    \"langchain_tool\",\n",
        "    \"you are an question-answering tool , you have to provide answers to your users precisely and mindfully according to the context provided to you\"\n",
        ")\n",
        "\n",
        "retriever_tool.run(\"what is query analysis\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJwXws27nTLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf873b6-6ead-487e-a27c-8588445ca2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia, sgmllib3k\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=a518fdb97a674d88e886de43d9bbfc1816af90aaf07304cb33a6453e5f68b288\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=ce48990a40b7ebf2304af04978b73f34271b311665c852ee7b49731c20ded107\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built wikipedia sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, wikipedia, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0 wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip  install --upgrade wikipedia arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxD0YQ5srGCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9fdec8-c60e-4443-e260-6c0852ca9d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: LangChain\n",
            "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
            "\n",
            "\n",
            "\n",
            "Page: Milvus (vector database)\n",
            "Summary: Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\n",
            "Milvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\n",
            "\n",
            "\n",
            "\n",
            "Page: Retrieval-augmented generation\n",
            "Summary: Retrieval Augmented Generation (RAG) is a technique that grants generative artificial intelligence models information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to augment information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.  \n",
            "Use cases include providing chatbot access to internal company data or giving factual information only from an authoritative source.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class WikiInputs(BaseModel):\n",
        "    \"\"\"Inputs to the wikipedia tool.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        description=\"query to look up in Wikipedia, should be 3 or less words\"\n",
        "    )\n",
        "\n",
        "\n",
        "wiki = WikipediaQueryRun(\n",
        "    name=\"wiki-tool\",\n",
        "    description=\"look up things in wikipedia\",\n",
        "    args_schema=WikiInputs,\n",
        "    api_wrapper=WikipediaAPIWrapper(),\n",
        "    return_direct=True,\n",
        ")\n",
        "\n",
        "print(wiki.run(\"langchain\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmZe6gT1oeLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "00bb098c-127a-4d7d-f7ab-ae8f95b87d68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Published: 2024-10-17\\nTitle: RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards\\nAuthors: Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong\\nSummary: Retrieval-Augmented Generation (RAG) has proven its effectiveness in\\nmitigating hallucinations in Large Language Models (LLMs) by retrieving\\nknowledge from external resources. To adapt LLMs for RAG pipelines, current\\napproaches use instruction tuning to optimize LLMs, improving their ability to\\nutilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses\\non equipping LLMs to handle diverse RAG tasks using different instructions.\\nHowever, it trains RAG modules to overfit training signals and overlooks the\\nvarying data preferences among agents within the RAG system. In this paper, we\\npropose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG\\nsystems by aligning data preferences between different RAG modules. DDR works\\nby collecting the rewards to optimize each agent with a rollout method. This\\nmethod prompts agents to sample some potential responses as perturbations,\\nevaluates the impact of these perturbations on the whole RAG system, and\\nsubsequently optimizes the agent to produce outputs that improve the\\nperformance of the RAG system. Our experiments on various knowledge-intensive\\ntasks demonstrate that DDR significantly outperforms the SFT method,\\nparticularly for LLMs with smaller-scale parameters that depend more on the\\nretrieved knowledge. Additionally, DDR exhibits a stronger capability to align\\nthe data preference between RAG modules. The DDR method makes generation module\\nmore effective in extracting key information from documents and mitigating\\nconflicts between parametric memory and external knowledge. All codes are\\navailable at https://github.com/OpenMatch/RAG-DDR.\\n\\nPublished: 2024-09-13\\nTitle: A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph\\nAuthors: Cheonsu Jeong\\nSummary: This study aims to improve knowledge-based question-answering (QA) systems by\\novercoming the limitations of existing Retrieval-Augmented Generation (RAG)\\nmodels and implementing an advanced RAG system based on Graph technology to\\ndevelop high-quality generative AI services. While existing RAG models\\ndemonstrate high accuracy and fluency by utilizing retrieved information, they\\nmay suffer from accuracy degradation as they generate responses using\\npre-loaded knowledge without reprocessing. Additionally, they cannot\\nincorporate real-time data after the RAG configuration stage, leading to issues\\nwith contextual understanding and biased information. To address these\\nlimitations, this study implemented an enhanced RAG system utilizing Graph\\ntechnology. This system is designed to efficiently search and utilize\\ninformation. Specifically, it employs LangGraph to evaluate the reliability of\\nretrieved information and synthesizes diverse data to generate more accurate\\nand enhanced responses. Furthermore, the study provides a detailed explanation\\nof the system's operation, key implementation steps, and examples through\\nimplementation code and validation results, thereby enhancing the understanding\\nof advanced RAG technology. This approach offers practical guidelines for\\nimplementing advanced RAG systems in corporate services, making it a valuable\\nresource for practical application.\\n\\nPublished: 2024-08-12\\nTitle: Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\\nAuthors: Fei Liu, Zejun Kang, Xing Han\\nSummary: With the growing demand for offline PDF chatbots in automotive industrial\\nproduction environments, optimizing the deployment of large language models\\n(LLMs) in local, low-performance settings has become increasingly important.\\nThis study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\\nfor processing complex automotive industry documents us\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain_community.tools import ArxivQueryRun\n",
        "from langchain_community.utilities import ArxivAPIWrapper\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class ArxivInputs(BaseModel):\n",
        "    \"\"\"Inputs to the Arxiv tool.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        description=\"query to look up in Arxiv, should be 3 or less words\"\n",
        "    )\n",
        "arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(), args_schema=ArxivInputs)\n",
        "\n",
        "\n",
        "arxiv.run(\"agents in Rag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVJu5y6Iujs_"
      },
      "outputs": [],
      "source": [
        "tools = [retriever_tool, arxiv , wiki]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict , Annotated , List , Sequence\n",
        "from langchain_core.messages import BaseMessage\n",
        "import operator\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ],
      "metadata": {
        "id": "9JtbyRq8-TC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kduoSDSesPuR"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_tool_calling_agent , AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder\n",
        "\n",
        "\n",
        "prompt= ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    (\"system\", \"You are an Agent assistant that have some additional tools , Answer the user queries{input} using these tools and generate natural language response\"),\n",
        "\n",
        "])\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "from langchain.schema import HumanMessage , AIMessage\n",
        "\n",
        "def agent_executor_node(state):\n",
        "  input_message = state[\"messages\"][-1].content\n",
        "  result = agent_executor.invoke({\"input\": input_message})\n",
        "  return {\"messages\": result[\"output\"]}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tools)\n",
        "\n",
        "def tool_call(state):\n",
        "  question = state[\"messages\"][-1].content\n",
        "  result = tool_executor.invoke(question)\n",
        "  return {\"messages\": result[\"output\"]}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0NSp-TK_zhB",
        "outputId": "1d9b4a1e-fd8f-45c0-cd64-c73245025781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-800b4fe953fb>:3: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
            "  tool_executor = ToolExecutor(tools)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph , END\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"agent_node\" , agent_executor_node)\n",
        "graph.add_node(\"tool_node\" , tool_call)\n",
        "\n",
        "graph.set_entry_point(\"agent_node\")\n",
        "graph.add_edge(\"agent_node\" , \"tool_node\")\n",
        "graph.add_edge(\"agent_node\" , END)\n",
        "\n",
        "app=graph.compile()"
      ],
      "metadata": {
        "id": "gRdIijS_GYC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseMessage , HumanMessage\n",
        "input = { \"messages\": [HumanMessage(content=\"how to make multi agent app using langraph?\")]}\n",
        "\n",
        "for output in app.stream(input, {\"recursion_limit\":10}):\n",
        "  for key,value in output.items():\n",
        "    print(f\"Get response from {key}\")\n",
        "    print(value)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "Kqm_EPlCJeYm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDvcSR7BocVllEcIDEyXZ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}